{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-11T03:04:43.049341Z",
     "start_time": "2025-02-11T03:04:43.044011Z"
    }
   },
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim as op\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomRotation, ToPILImage, Resize, ColorJitter, Normalize, ToTensor\n",
    "import os\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:05:01.056113Z",
     "start_time": "2025-02-11T03:05:01.053578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = r'D:\\Python\\Projects\\CV\\Computer_Vision\\FaceNet\\face_test\\train\\AI1904'\n",
    "batch_size = 16\n",
    "epoch = 5\n",
    "workers = 0 if os.name == 'nt' else 8"
   ],
   "id": "ff5089e7c320a1f4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:05:01.651634Z",
     "start_time": "2025-02-11T03:05:01.647799Z"
    }
   },
   "cell_type": "code",
   "source": "device = 'cuda' if torch.cuda.is_available() else 'cpu'",
   "id": "e7ad4b346b8685f4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:05:01.989069Z",
     "start_time": "2025-02-11T03:05:01.975971Z"
    }
   },
   "cell_type": "code",
   "source": "mtcnn = MTCNN(device=device, margin=10, keep_all=True, thresholds=[0.5, 0.6, 0.6])",
   "id": "f69d94e0d9e0168",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:05:04.809319Z",
     "start_time": "2025-02-11T03:05:02.979372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = datasets.ImageFolder(path, transform=transforms.Resize((512,512)))  # you can use `PIL.Image.resize` if you want\n",
    "\n",
    "# After it reads the folder people, it will asign each folder from 0-inf this line of code save the new cropped image to the same folder. It is not nessasary to run the code\n",
    "dataset.samples = [(p, p.replace(path, path + 'cropped')) for p, _ in dataset.samples]\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=batch_size, num_workers=workers, collate_fn=training.collate_pil)\n",
    "for i, (x,y) in enumerate(loader):\n",
    "    try:\n",
    "        mtcnn(x, save_path=y)\n",
    "        print('\\rBatch {} of {}'.format(i + 1, len(loader)), end='')\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "# Remove mtcnn to reduce GPU memory usage\n",
    "del mtcnn"
   ],
   "id": "a264e0d7a0202daf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 of 3"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:06:15.704394Z",
     "start_time": "2025-02-11T03:06:15.393907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remember so set classify=False if you want to add new people in the future, or do one-shot face matching\n",
    "temp_dataset = datasets.ImageFolder(r'D:\\Python\\Projects\\CV\\Computer_Vision\\FaceNet\\face_test\\train\\AI1904')\n",
    "resnet = InceptionResnetV1(pretrained='vggface2', classify=True, num_classes=len(temp_dataset.class_to_idx)).to(device)"
   ],
   "id": "88ff8c291c595eea",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:06:16.424541Z",
     "start_time": "2025-02-11T03:06:16.419582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = op.AdamW(resnet.parameters(), lr=0.001)\n",
    "scl = MultiStepLR(optimizer, [5,10])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "metric = {'fps': training.BatchTimer(), 'acc': training.accuracy}"
   ],
   "id": "da7f9f0f4f444a17",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:07:25.756806Z",
     "start_time": "2025-02-11T03:07:25.751605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_path = r'D:\\Python\\Projects\\CV\\Computer_Vision\\FaceNet\\face_test\\train\\AI1904'\n",
    "val_path = r'D:\\Python\\Projects\\CV\\Computer_Vision\\FaceNet\\face_test\\test\\AI1904'\n",
    "\n",
    "train_trans = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.RandomHorizontalFlip(0.3),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4792, 0.4191, 0.3912],\n",
    "                         std=[0.2922, 0.2726, 0.2682]),\n",
    "    fixed_image_standardization\n",
    "])\n",
    "\n",
    "val_trans = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4792, 0.4191, 0.3912],\n",
    "                         std=[0.2922, 0.2726, 0.2682]),\n",
    "    fixed_image_standardization\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_dataset = datasets.ImageFolder(train_path, transform=train_trans)\n",
    "val_dataset   = datasets.ImageFolder(val_path, transform=val_trans)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=workers,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=workers,\n",
    "    shuffle=False\n",
    ")"
   ],
   "id": "1b81f157581efcfb",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:07:57.498031Z",
     "start_time": "2025-02-11T03:07:51.153876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "def crop_and_save_faces(input_dir, output_dir, mtcnn):\n",
    "    dataset = datasets.ImageFolder(input_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for label, class_name in enumerate(dataset.classes):\n",
    "        os.makedirs(os.path.join(output_dir, class_name), exist_ok=True)\n",
    "\n",
    "    for img_path, label in tqdm(dataset.imgs, desc=f\"Processing {input_dir}\"):\n",
    "        img_name = os.path.basename(img_path)\n",
    "        save_path = os.path.join(output_dir, dataset.classes[label], img_name)\n",
    "\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            cropped_img = mtcnn(img)\n",
    "            if cropped_img is not None:\n",
    "                ToPILImage()(cropped_img).save(save_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "train_path_cropped = './data/train_cropped'\n",
    "val_path_cropped = './data/val_cropped'\n",
    "\n",
    "mtcnn = MTCNN(device=device, margin=10, keep_all=False, thresholds=[0.5, 0.6, 0.6])\n",
    "\n",
    "crop_and_save_faces(train_path, train_path_cropped, mtcnn)\n",
    "crop_and_save_faces(val_path, val_path_cropped, mtcnn)"
   ],
   "id": "ffd41bb5118dbd9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing D:\\Python\\Projects\\CV\\Computer_Vision\\FaceNet\\face_test\\train\\AI1904: 100%|██████████| 18/18 [00:01<00:00, 10.45it/s]\n",
      "Processing D:\\Python\\Projects\\CV\\Computer_Vision\\FaceNet\\face_test\\test\\AI1904: 100%|██████████| 46/46 [00:04<00:00, 10.00it/s]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T03:08:12.001837Z",
     "start_time": "2025-02-11T03:08:01.781930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "writer = SummaryWriter()\n",
    "writer.iteration, writer.interval = 0, 10\n",
    "\n",
    "print('\\n\\nInitial')\n",
    "print('-' * 10)\n",
    "resnet.eval()\n",
    "training.pass_epoch(\n",
    "    resnet, loss_fn, val_loader,\n",
    "    batch_metrics=metric, show_running=True, device=device,\n",
    "    writer=writer\n",
    ")\n",
    "\n",
    "for i in range(epoch):\n",
    "    print('\\nEpoch {}/{}'.format(i + 1, epoch))\n",
    "    print('-' * 10)\n",
    "\n",
    "    resnet.train()\n",
    "    training.pass_epoch(\n",
    "        resnet, loss_fn, train_loader, optimizer, scl,\n",
    "        batch_metrics=metric, show_running=True, device=device,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "    resnet.eval()\n",
    "    training.pass_epoch(\n",
    "        resnet, loss_fn, val_loader,\n",
    "        batch_metrics=metric, show_running=True, device=device,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "writer.close()"
   ],
   "id": "ee05c915c382f10a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Initial\n",
      "----------\n",
      "Valid |     3/3    | loss:    1.4810 | fps:   33.2061 | acc:    0.5536   \n",
      "\n",
      "Epoch 1/5\n",
      "----------\n",
      "Train |     2/2    | loss:    1.1247 | fps:   13.2917 | acc:    0.6562   \n",
      "Valid |     3/3    | loss:    1.9384 | fps:   53.2952 | acc:    0.0625   \n",
      "\n",
      "Epoch 2/5\n",
      "----------\n",
      "Train |     2/2    | loss:    0.8857 | fps:   15.1764 | acc:    0.5938   \n",
      "Valid |     3/3    | loss:    8.1314 | fps:   54.3871 | acc:    0.5536   \n",
      "\n",
      "Epoch 3/5\n",
      "----------\n",
      "Train |     2/2    | loss:    1.8821 | fps:   15.2462 | acc:    0.4688   \n",
      "Valid |     3/3    | loss:    8.4809 | fps:   54.6365 | acc:    0.5536   \n",
      "\n",
      "Epoch 4/5\n",
      "----------\n",
      "Train |     2/2    | loss:    0.6300 | fps:   15.1277 | acc:    0.7500   \n",
      "Valid |     3/3    | loss:    3.2010 | fps:   54.8970 | acc:    0.5536   \n",
      "\n",
      "Epoch 5/5\n",
      "----------\n",
      "Train |     2/2    | loss:    0.2832 | fps:   15.0274 | acc:    0.9375   \n",
      "Valid |     3/3    | loss:    4.8578 | fps:   55.4753 | acc:    0.0833   \n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T11:16:17.318913Z",
     "start_time": "2025-01-23T11:16:17.315270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # This code does the same thing as above, not sure why I wrote it\n",
    "# for epoch_num in range(epoch):\n",
    "#     train_loss = 0\n",
    "#     resnet.train()\n",
    "#\n",
    "#     for batch, (x_train, y_train) in enumerate(train_loader):\n",
    "#         x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "#         y_pred = resnet(x_train)\n",
    "#         loss = loss_fn(y_pred, y_train)\n",
    "#         train_loss += loss.item()\n",
    "#\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     train_loss /= len(train_loader)\n",
    "#\n",
    "#     test_loss = 0\n",
    "#     resnet.eval()\n",
    "#     with torch.inference_mode():\n",
    "#         for X_test, y_test in val_loader:\n",
    "#             X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "#             test_pred = resnet(X_test)\n",
    "#             test_loss += loss_fn(test_pred, y_test).item()\n",
    "#\n",
    "#     test_loss /= len(val_loader)\n",
    "#     print(f'Epoch {epoch_num + 1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ],
   "id": "33a64d56cdb8a9b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save model",
   "id": "5236d38494c48634"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T11:16:17.396842Z",
     "start_time": "2025-01-23T11:16:17.331095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save({\n",
    "    'model_state_dict': resnet.state_dict(),\n",
    "    'class_to_idx': dataset.class_to_idx  # Save label-to-class mapping\n",
    "}, '../model.pth')"
   ],
   "id": "3182633edee9099b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load model",
   "id": "e288f63d62204374"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T11:16:17.442501Z",
     "start_time": "2025-01-23T11:16:17.398535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the saved model\n",
    "checkpoint = torch.load('../model.pth', map_location=device)  # Use map_location='cpu' if no GPU available\n",
    "resnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "resnet.eval()  # Put the model in evaluation mode\n",
    "print('Model loaded and ready for inference')"
   ],
   "id": "cc02fc5931ea4a4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready for inference\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Finding the mean and std of the dataset",
   "id": "22d1729b6f5f80d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T11:16:18.188173Z",
     "start_time": "2025-01-23T11:16:17.809715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset WITHOUT applying normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),  # Ensure dimensions match model input\n",
    "    transforms.ToTensor()  # Convert to Tensor (values in range [0,1])\n",
    "])\n",
    "dataset = datasets.ImageFolder('./train', transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Calculate mean and std\n",
    "mean = 0.0\n",
    "std = 0.0\n",
    "n_samples = 0\n",
    "for images, _ in loader:\n",
    "    n_samples += images.size(0)  # Add the batch size\n",
    "    images = images.view(images.size(0), images.size(1), -1)  # Flatten HxW to single dimension\n",
    "    mean += images.mean(2).sum(0)  # Sum mean per channel\n",
    "    std += images.std(2).sum(0)  # Sum std per channel\n",
    "\n",
    "mean /= n_samples\n",
    "std /= n_samples\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std: {std}\")\n"
   ],
   "id": "806d486a51b8cd08",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './train'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 10\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Define the dataset WITHOUT applying normalization\u001B[39;00m\n\u001B[0;32m      6\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[0;32m      7\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mResize((\u001B[38;5;241m160\u001B[39m, \u001B[38;5;241m160\u001B[39m)),  \u001B[38;5;66;03m# Ensure dimensions match model input\u001B[39;00m\n\u001B[0;32m      8\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor()  \u001B[38;5;66;03m# Convert to Tensor (values in range [0,1])\u001B[39;00m\n\u001B[0;32m      9\u001B[0m ])\n\u001B[1;32m---> 10\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mdatasets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mImageFolder\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./train\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m loader \u001B[38;5;241m=\u001B[39m DataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Calculate mean and std\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Python\\Projects\\Community\\venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:309\u001B[0m, in \u001B[0;36mImageFolder.__init__\u001B[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001B[0m\n\u001B[0;32m    301\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m    302\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    303\u001B[0m     root: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    307\u001B[0m     is_valid_file: Optional[Callable[[\u001B[38;5;28mstr\u001B[39m], \u001B[38;5;28mbool\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    308\u001B[0m ):\n\u001B[1;32m--> 309\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    310\u001B[0m \u001B[43m        \u001B[49m\u001B[43mroot\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mIMG_EXTENSIONS\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mis_valid_file\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtarget_transform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget_transform\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_valid_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_valid_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimgs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamples\n",
      "File \u001B[1;32mD:\\Python\\Projects\\Community\\venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:144\u001B[0m, in \u001B[0;36mDatasetFolder.__init__\u001B[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001B[0m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m    135\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    136\u001B[0m     root: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    141\u001B[0m     is_valid_file: Optional[Callable[[\u001B[38;5;28mstr\u001B[39m], \u001B[38;5;28mbool\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    142\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(root, transform\u001B[38;5;241m=\u001B[39mtransform, target_transform\u001B[38;5;241m=\u001B[39mtarget_transform)\n\u001B[1;32m--> 144\u001B[0m     classes, class_to_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind_classes\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroot\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    145\u001B[0m     samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_dataset(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot, class_to_idx, extensions, is_valid_file)\n\u001B[0;32m    147\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader \u001B[38;5;241m=\u001B[39m loader\n",
      "File \u001B[1;32mD:\\Python\\Projects\\Community\\venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:218\u001B[0m, in \u001B[0;36mDatasetFolder.find_classes\u001B[1;34m(self, directory)\u001B[0m\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfind_classes\u001B[39m(\u001B[38;5;28mself\u001B[39m, directory: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[List[\u001B[38;5;28mstr\u001B[39m], Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mint\u001B[39m]]:\n\u001B[0;32m    192\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001B[39;00m\n\u001B[0;32m    193\u001B[0m \n\u001B[0;32m    194\u001B[0m \u001B[38;5;124;03m        directory/\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001B[39;00m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfind_classes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdirectory\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python\\Projects\\Community\\venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:40\u001B[0m, in \u001B[0;36mfind_classes\u001B[1;34m(directory)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfind_classes\u001B[39m(directory: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[List[\u001B[38;5;28mstr\u001B[39m], Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mint\u001B[39m]]:\n\u001B[0;32m     36\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \n\u001B[0;32m     38\u001B[0m \u001B[38;5;124;03m    See :class:`DatasetFolder` for details.\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 40\u001B[0m     classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(entry\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscandir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdirectory\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m entry\u001B[38;5;241m.\u001B[39mis_dir())\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m classes:\n\u001B[0;32m     42\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find any class folder in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdirectory\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] The system cannot find the path specified: './train'"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T11:16:18.190995400Z",
     "start_time": "2025-01-23T06:14:54.528306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "def verify_images(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            # Try opening the image\n",
    "            with Image.open(file_path) as img:\n",
    "                img.verify()  # Verify that the image fits the PIL format\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print(f\"Corrupted or unsupported file: {file_path}\")\n",
    "            # Optionally, remove the corrupted file\n",
    "            # os.remove(file_path)\n",
    "\n",
    "\n",
    "# Validate images in 'Me_val' and 'madonna' folders\n",
    "verify_images('data/train/Me')\n",
    "verify_images('data/train/madonna')\n"
   ],
   "id": "536178ae0405165e",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T11:16:18.190995400Z",
     "start_time": "2025-01-23T06:15:32.001305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = 'data/train'\n",
    "for folder in os.listdir(dataset_path):\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    if os.path.isdir(folder_path):  # Skip non-directory files\n",
    "        verify_images(folder_path)\n"
   ],
   "id": "17495e29b87569f2",
   "outputs": [],
   "execution_count": 45
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
